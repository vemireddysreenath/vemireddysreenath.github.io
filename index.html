<!DOCTYPE html>
<!-- upto 2 directory depth--> 
<html lang="en-US">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-K8NFKJ9RBX"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-K8NFKJ9RBX');
</script>
    
    
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Sreenath profile</title>
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="crossorigin"/>
    <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Nunito+Sans:wght@300;400;700;800&amp;display=swap"/>
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Nunito+Sans:wght@300;400;700;800&amp;display=swap" media="print" onload="this.media='all'"/>
    <noscript>
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Nunito+Sans:wght@300;400;700;800&amp;display=swap"/>
    </noscript>
    <link href="./css/font-awesome/css/all.min.css?ver=1.2.0" rel="stylesheet">
    <link href="./css/bootstrap-icons/bootstrap-icons.css?ver=1.2.0" rel="stylesheet">
    <link href="./css/bootstrap.min.css?ver=1.2.0" rel="stylesheet">
    <link href="./css/aos.css?ver=1.2.0" rel="stylesheet">
    <link href="./css/main.css?ver=1.2.0" rel="stylesheet">
    <noscript>
      <style type="text/css">
        [data-aos] {
            opacity: 1 !important;
            transform: translate(0) scale(1) !important;
        }
      </style>
    </noscript>
  </head>
  <body id="top">
    <header class="bg-light">
      <nav class="navbar navbar-expand-lg navbar-light bg-light" id="header-nav" role="navigation">
        <div class="container"><a class="link-dark navbar-brand site-title mb-0" href="#">Profile</a>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
          <div class="collapse navbar-collapse" id="navbarSupportedContent">
            <ul class="navbar-nav ms-auto me-2">
              <li class="nav-item"><a class="nav-link" href="#about">About</a></li>
              <li class="nav-item"><a class="nav-link" href="#Summary">Summary</a></li>
              <li class="nav-item"><a class="nav-link" href="#skills">Skills</a></li>
              <li class="nav-item"><a class="nav-link" href="#experience">Experience</a></li>
              <li class="nav-item"><a class="nav-link" href="#Projects">Projects</a></li>
              <li class="nav-item"><a class="nav-link" href="#contact">Contact</a></li>
            </ul>
          </div>
        </div>
      </nav>
    </header>
    <div class="page-content">
      <div id="content">
<header>
  <div class="cover bg-light">
    <div class="container px-3">
      <div class="row">
        <div class="col-lg-6 p-2"><img class="img-fluid" src="images/illustrations/hello3.svg" alt="hello"/></div>
        <div class="col-lg-6">
          <div class="mt-5">
            <p class="lead text-uppercase mb-1">Hello!</p>
            <h1 class="intro-title marker" data-aos="fade-left" data-aos-delay="50">I’m Sreenath Vemireddy</h1>
            <p class="lead fw-normal mt-3" data-aos="fade-up" data-aos-delay="100">Sr. BigData Developer</p>
            <div class="social-nav" data-aos="fade-up" data-aos-delay="200">
              <nav role="navigation">
                <ul class="nav justify-content-left">
                  <li class="nav-item"><a class="nav-link" href="https://www.linkedin.com/in/sreenath-reddy-vemireddy-09bb91183" title="LinkedIn"><i class="fab fa-linkedin"></i><span class="menu-title sr-only">LinkedIn</span></a></li>
                  <li class="nav-item"><a class="nav-link" href="https://wa.me/14697933883" title="+1 (469) 793-3883"><i class='fab fa-whatsapp-square'></i><span class="menu-title sr-only">WhatsApp</span></a></li>
                  <li class="nav-item"><a class="nav-link" href="https://raw.githubusercontent.com/vemireddysreenath/vemireddysreenath.github.io/main/SREENATH_RESUME.docx" title="Latest Resume"><i class='fas fa-download'></i><span class="menu-title sr-only">Download Resume</span></a></li>

                </ul>
              </nav>
            </div>
            <div class="mt-3" data-aos="fade-up" data-aos-delay="200"><a class="btn btn-primary shadow-sm mt-1 hover-effect" href="#contact">Get In Touch <i class="fas fa-arrow-right"></i></a></div>
          </div>
        </div>
      </div>
    </div>
  </div>
  <div class="wave-bg"></div>
</header>
<div class="section pt-4 px-3 px-lg-4" id="about">
  <div class="container-narrow">
    <div class="row">
      <div class="col-md-6">
        <h2 class="h4 my-2">Hello! I’m Sreenath Vemireddy.</h2>
        <p> I am a <b>Senior Big Data and Azure Data Engineer</b> with over <b>10 years of progressive experience</b> in designing, building, and managing end-to-end data engineering solutions across diverse industries including <b>finance, healthcare, insurance, banking, and geospatial mapping</b>. My core strength lies in building <b>scalable, high-performance data pipelines</b> using cutting-edge technologies like <b>Hadoop, Spark, PySpark, and Hive</b>, and integrating them seamlessly with <b>cloud platforms such as Microsoft Azure and AWS</b>.</p>
        
      </div>
      <div class="col-md-5 offset-md-1" data-aos="fade-left" data-aos-delay="100"><img class="avatar img-fluid mt-2" src="images/avatar.png" width="400" height="400" alt="Sreenath Vemireddy"/></div>
    </div>
  </div>
</div>
<div class="section px-3 px-lg-4 pt-5" id="Summary">
  <div class="container-narrow">
    <div class="text-center mb-5">
      <h2 class="marker marker-center">Proficianal Summary</h2>
    </div>
    <div class="text-center">
        <p class="mx-auto mb-3" style="max-width:850px"> I have a proven track record in delivering <b>data migrations</b>—moving terabytes of data from legacy systems to cloud environments—while ensuring <b>data quality, lineage tracking, regulatory compliance</b>, and performance optimization. I specialize in <b>metadata-driven automation</b> frameworks for <b>schema validation, ingestion, transformation, audit logging, and alerting</b>, significantly reducing manual effort and enhancing scalability.</p>
        <p class="mx-auto mb-3" style="max-width:850px"> My expertise includes tools like <b>Azure Data Factory (ADF)</b>, <b>Azure Blob Storage</b>, <b>Azure SQL Database</b>, <b>Azure Key Vault</b>, and orchestration tools like <b>Apache Airflow, Zena, and Autosys</b>. I have implemented <b>complex DAGs</b> for ETL pipelines, automated failure handling, and ensured secure cloud integration with strict governance practices.</p>
        <p class="mx-auto mb-3" style="max-width:850px"> I’ve collaborated with top-tier global organizations such as <b>DBS Bank, PayPal, ICICI Bank, AbbVie, Country Financial, Apple, and Nokia</b>. My work has contributed to mission-critical systems including <b>MAS 637 and PILLAR3 regulatory reporting</b>, <b>SAS exit transformations</b>, <b>CRM cloud migrations</b>, <b>HANA-to-Hadoop reporting</b>, and <b>spatial data integration for Apple and Nokia Maps</b>.</p>
        <p class="mx-auto mb-3" style="max-width:850px"> I am well-versed in <b>data governance</b> practices, using tools like <b>Collibra</b> to manage metadata and ensure compliance. I regularly optimize performance of <b>Spark jobs, Hive queries, and ADF pipelines</b> and have utilized <b>Power BI, Adobe Analytics, and Apache Superset</b> for data visualization and reporting.</p>
        <p class="mx-auto mb-3" style="max-width:850px"> My approach is guided by <b>Agile methodologies</b> and <b>DevOps principles</b>, including <b>CI/CD automation with Git and Azure DevOps</b>. I’ve also taken on <b>leadership roles</b>, mentoring junior engineers, conducting code reviews, and guiding teams in the delivery of scalable and robust solutions.</p>
        <p class="mx-auto mb-3" style="max-width:850px"> I am passionate about <b>transforming raw data into actionable intelligence</b> and continuously strive to enhance data value, accessibility, and governance across organizations. For me, data engineering is not just about infrastructure—it's about enabling smarter, data-driven decisions that drive business success.</p>
        <p class="mx-auto mb-3" style="max-width:850px">I am currently serving as a <b>Cloud Data Engineer</b> at <b>Country Financial</b>, where I lead end-to-end Azure-based data migration projects integrating Databricks, Delta Lake, and metadata-driven automation. I’ve also championed the integration of <b>Collibra</b> with enterprise pipelines to automate lineage and data quality profiling for regulatory compliance.</p>
    </div>
    <!-- Removed the symbol code -->
    </div>
  </div>
</div>
<div class="section px-3 px-lg-4 pt-5" id="skills">
  <div class="container-narrow">
    <div class="text-center mb-5">
      <h2 class="marker marker-center">My Skills</h2>
    </div>
    <!-- commented 
    <div class="text-center">
      <p class="mx-auto mb-3" style="max-width:600px">I am a quick learner and specialize in multitude of skills required for Web Application Development and Product Design</p>
    </div>
    Commented-->
    <div class="bg-light p-3">
      <div class="row">
        <div class="col-md-50">
        
          <!-- new lines added for skill -->
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">Hadoop, Spark, PySpark</span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">Cloudera, Hortonworks</span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">Python, Scala, Core Java, SQL, Shell Scripting</span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">Hue, Apache Superset, Hive, Impala, Pig, Sqoop</span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">Azure Data Factory, Blob Storage, SQL Database, Key Vault, Data Lake, Delta Lake, CasmosDB, Databricks, Snowflake </span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">AWS (S3, Ingestion Pipelines)</span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">Apache Airflow, Zena, Autosys (JIL scripting)</span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">MySQL, Oracle, SQL Server, Vertica, SAP HANA, MongoDB, Presto</span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">Adobe Analytics, Power BI, SAS, Watch UI</span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">PyCharm, IntelliJ, Eclipse, Jupyter Notebook, Azure DevOps, IntelliJ, Eclipse</span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">Git, GitHub, Bitbucket</span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">Metadata-Driven Automation Frameworks, ADA Framework</span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">CI/CD Pipelines, Agile Delivery, DevOps Practices</span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">Collibra (Data Governance), MAS 637, PILLAR3 Reporting</span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">Metadata-Driven Python Frameworks, ADA Framework, Data Modeling, ETL Design</span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">TEZ, Click Stream Data, Azure Logic Apps, Unity Catalog, Custom PayPal Frameworks</span></div>
          </div>

        </div>
      </div>
    </div>
  </div>
</div>

<div class="section px-3 px-lg-4 pt-5" id="Projects">
  <div class="container-narrow">
    <div class="text-center mb-5">
      <h2 class="marker marker-center">Latest Projects</h2>
    </div>
    <div class="row">
      <!-- Updated Project cards -->
      <!-- Project 1: Comm-Agg End-to-End Data Migration Solution -->
      <div class="col-md-12">
        <div class="card mb-3" data-aos="fade-right" data-aos-delay="200">
          <div class="card-header px-3 py-2">
            <div class="d-flex justify-content-between">
              <div>
                <h3 class="h5 mb-1">Comm-Agg End-to-End Data Migration Solution</h3>
                <div class="text-muted text-small"><b>Client:</b> Country Financial</div>
              </div><small>Jan 2024 – Present</small>
            </div>
          </div>
          <div class="card-body px-3 py-2">
            <p><b>Description:</b> Designed and developed a metadata‑driven, end‑to‑end data migration and analytics platform to ingest data from Guidewire S3 into Azure SQL, Data Lake, and Databricks Delta Lake, enabling scalable analytics, historical tracking, and regulatory compliance.</p>
            <ul>
              <li>Designed and implemented a metadata-driven Azure Data Factory (ADF) pipeline framework for ingestion from Guidewire S3 buckets to Azure SQL, Data Lake, and Databricks Delta Lake, supporting dynamic schema evolution and schema validation.</li>
              <li>Built modular PySpark transformation jobs in Databricks using Delta Lake, implementing SCD Type 2 logic, historical tracking, and late arriving dimension support.</li>
              <li>Developed parameterized and conditional orchestration using Zena scheduler, enabling seamless pipeline integration based on metadata inputs and business rules.</li>
              <li>Integrated Collibra APIs for automated metadata registration, lineage capture, and data quality profiling of ingested datasets, supporting regulatory compliance.</li>
              <li>Developed comprehensive audit logging, alerting, and reconciliation layers using PySpark, ADF logging tables, and automated email notifications to ensure data integrity and compliance.</li>
              <li>Optimized Spark job performance for large Guidewire datasets (~20TB) by tuning shuffle partitions, caching intermediate results, and optimizing complex join operations.</li>
              <li>Conducted thorough end-to-end unit testing and data validation using PySpark and SQL, reconciling with legacy Oracle systems to ensure fidelity and completeness of migrated data.</li>
              <li>Mentored and guided junior engineers, conducted code reviews, and led weekly sprint demos with product owners and business SMEs to ensure alignment and quality delivery.</li>
              <li>Enabled business users to visualize key insurance metrics by building Power BI dashboards consuming curated Delta Lake tables, improving data accessibility and decision making.</li>
              <li>Automated schema drift handling and late arriving data management, reducing manual intervention and ensuring continuous pipeline robustness.</li>
            </ul>
            <p><b>Technologies Used:</b> Azure Data Factory (ADF), Azure Data Lake Storage Gen2, Delta Lake, Databricks, Blob Storage, Azure SQL, Python, Hadoop, Hive, Impala, Zena, Shell Scripting</p>
          </div>
        </div>
      </div>
      <!-- Project 2a: MAS 637 Reporting -->
      <div class="col-md-12">
        <div class="card mb-3" data-aos="fade-left" data-aos-delay="200">
          <div class="card-header px-3 py-2">
            <div class="d-flex justify-content-between">
              <div>
                <h3 class="h5 mb-1">MAS 637 Reporting</h3>
                <div class="text-muted text-small"><b>Client:</b> DBS Bank</div>
              </div><small>Jun 2022 – Jan 2024</small>
            </div>
          </div>
          <div class="card-body px-3 py-2">
            <p><b>Description:</b> The MAS 637 project involved generating reports for the Monetary Authority of Singapore (MAS) to ensure regulatory compliance. The solution analyzed pool statuses across various user types and tracked changes over a 12‑month period.</p>
            <ul>
              <li>Designed and developed regulatory reporting pipelines for MAS 637 leveraging PySpark within the ADA (Automated Data Analytics) framework to automate ingestion, transformation, and aggregation of large-scale financial datasets.</li>
              <li>Integrated end-to-end data lineage and data quality controls using Collibra and custom Python validation scripts, enabling traceability and compliance for all reportable data elements.</li>
              <li>Collaborated with risk, compliance, and business teams to define regulatory requirements, ensuring accurate and timely submission of MAS 637 reports to Singapore authorities.</li>
              <li>Optimized Presto and Hive queries to efficiently process high-volume transactional data, reducing reporting cycle times and improving query performance.</li>
              <li>Provided insights into financial transactions and default status by integrating multiple internal data sources, improving transparency for auditors and stakeholders.</li>
              <li>Delivered high‑quality, compliant reports to the Monetary Authority of Singapore, ensuring all data and analysis met strict regulatory standards.</li>
              <li>Developed automated alerting mechanisms for SLA breaches and compliance exceptions using Airflow DAGs and email triggers, ensuring proactive remediation and transparency.</li>
              <li>Documented all data flows, transformations, and control points to support internal and external audits for regulatory purposes.</li>
            </ul>
            <p><b>Technologies Used:</b> ADA&nbsp;(In‑house&nbsp;framework),&nbsp;PySpark,&nbsp;Spark&nbsp;SQL,&nbsp;Presto,&nbsp;Hive,&nbsp;Hadoop,&nbsp;Airflow,&nbsp;Jupyter,&nbsp;Collibra,&nbsp;Python,&nbsp;Shell&nbsp;Scripting</p>
          </div>
        </div>
      </div>
      <!-- Project 2b: PILLAR3 Risk Analytics -->
      <div class="col-md-12">
        <div class="card mb-3" data-aos="fade-left" data-aos-delay="200">
          <div class="card-header px-3 py-2">
            <div class="d-flex justify-content-between">
              <div>
                <h3 class="h5 mb-1">PILLAR3 Risk Analytics</h3>
                <div class="text-muted text-small"><b>Client:</b> DBS Bank</div>
              </div><small>Jun 2022 – Jan 2024</small>
            </div>
          </div>
          <div class="card-body px-3 py-2">
            <p><b>Description:</b> PILLAR3 reporting focused on risk management, using predictive analytics to evaluate user behavior and default risks based on historical data.</p>
            <ul>
              <li>Developed advanced analytics and dashboards for PILLAR3 risk exposure using PySpark, Presto, and Apache Superset, enabling real-time risk monitoring for senior management.</li>
              <li>Designed and implemented historical snapshot logic for 12-month risk trend analysis, supporting regulatory and business reporting requirements.</li>
              <li>Enabled interactive drill-down and slice-and-dice capabilities for senior management and regulatory auditors, improving transparency and decision support.</li>
              <li>Implemented robust data masking, row-level security, and access controls to ensure strict adherence to data privacy and security guidelines.</li>
              <li>Collaborated with data governance teams to define and implement Collibra data quality (DQ) rules and lineage modeling for all critical risk datasets.</li>
              <li>Automated validation and exception handling workflows, reducing manual effort and increasing data reliability for risk analytics.</li>
              <li>Applied predictive analytics techniques to evaluate potential default risks, improving the bank’s overall risk‑management strategy.</li>
            </ul>
            <p><b>Technologies Used:</b> PySpark,&nbsp;Spark&nbsp;SQL,&nbsp;Presto,&nbsp;Apache&nbsp;Superset,&nbsp;Hive,&nbsp;Collibra,&nbsp;Airflow</p>
          </div>
        </div>
      </div>
      <!-- Project 3: SAS Exit -->
      <div class="col-md-12">
        <div class="card mb-3" data-aos="fade-right" data-aos-delay="200">
          <div class="card-header px-3 py-2">
            <div class="d-flex justify-content-between">
              <div>
                <h3 class="h5 mb-1">SAS Exit</h3>
                <div class="text-muted text-small"><b>Client:</b> DBS Bank</div>
              </div><small>Jun 2022 – Jan 2024</small>
            </div>
          </div>
          <div class="card-body px-3 py-2">
            <p><b>Description:</b> The SAS Exit project was designed to migrate legacy SAS scripts into DBS Bank’s ADA platform. The migration converted complex SAS reports into PySpark‑based solutions, improving performance and scalability of the reporting system.</p>
            <ul>
              <li>Reverse-engineered and migrated 100+ legacy SAS reporting scripts and ETL workflows to PySpark and Spark SQL within the ADA framework, ensuring full functional parity and improved scalability.</li>
              <li>Refactored monolithic SAS jobs into modular, reusable Spark components integrated with Airflow orchestration for automated scheduling and monitoring.</li>
              <li>Developed comprehensive validation, reconciliation, and audit scripts to ensure data accuracy and completeness post-migration, supporting regulatory and business reporting requirements.</li>
              <li>Reduced end-to-end report processing time by 50% and eliminated recurring SAS licensing costs, delivering significant cost savings for the organization.</li>
              <li>Provided detailed documentation, knowledge transfer, and hands-on training to analytics teams and business users on the new PySpark-based reporting framework.</li>
              <li>Collaborated with compliance and data‑governance teams to define domain‑specific policies, ensuring consistent rule enforcement across systems.</li>
              <li>Integrated Collibra with bank‑wide data pipelines to automate lineage tracking and data‑quality profiling.</li>
              <li>Enabled dynamic parameterization and metadata-driven job execution, increasing flexibility and reducing manual intervention.</li>
            </ul>
            <p><b>Technologies Used:</b> SAS, ADA (In-house framework), PySpark, Airflow, Collibra, Python, Spark SQL, Presto, Hive, Hadoop</p>
          </div>
        </div>
      </div>
      <!-- Project 4: CRM Data Migration -->
      <div class="col-md-12">
        <div class="card mb-3" data-aos="fade-left" data-aos-delay="200">
          <div class="card-header px-3 py-2">
            <div class="d-flex justify-content-between">
              <div>
                <h3 class="h5 mb-1">CRM Data Migration</h3>
                <div class="text-muted text-small"><b>Client:</b> ICICI</div>
              </div><small>Jan 2021 – Jun 2022</small>
            </div>
          </div>
          <div class="card-body px-3 py-2">
            <p><b>Description:</b> This project involved migrating CRM data from multiple on‑premises and cloud environments to an Azure‑based platform with the goal of ensuring smooth data transfer while generating insightful business reports.</p>
            <ul>
              <li>Designed and implemented Azure Data Factory (ADF) pipelines to extract, transform, and load CRM data from diverse on-premises (Oracle, SQL Server) and cloud sources into Azure SQL, Blob Storage, and Data Lake.</li>
              <li>Integrated Azure Key Vault for secure credential management, and implemented automated error handling and notification workflows using Azure Logic Apps.</li>
              <li>Developed robust schema validation, data profiling, and reconciliation scripts to ensure data quality, consistency, and completeness throughout the migration process.</li>
              <li>Built interactive Power BI dashboards and custom SQL reports to enable business stakeholders to visualize CRM metrics, trends, and KPIs in real time.</li>
              <li>Coordinated with cross-functional business, IT, and QA teams to ensure zero data loss, minimal downtime, and seamless business continuity during cutover and migration phases.</li>
              <li>Documented all migration processes, controls, and data mappings, supporting compliance and audit requirements.</li>
            </ul>
            <p><b>Technologies Used:</b> Azure Data Factory (ADF), Blob Storage, SQL Database, Oracle, Key-Vault, Vertica, Azure Logic Apps</p>
          </div>
        </div>
      </div>
      <!-- Project 5: H2H (HANA to Hadoop) -->
      <div class="col-md-12">
        <div class="card mb-3" data-aos="fade-right" data-aos-delay="200">
          <div class="card-header px-3 py-2">
            <div class="d-flex justify-content-between">
              <div>
                <h3 class="h5 mb-1">H2H (HANA to Hadoop)</h3>
                <div class="text-muted text-small"><b>Client:</b> PayPal</div>
              </div><small>Jul 2020 – Jan 2022</small>
            </div>
          </div>
          <div class="card-body px-3 py-2">
            <p><b>Description:</b> The H2H project involved migrating SAP HANA reports to a Hadoop‑based environment, ensuring improved performance and scalability for reporting and analytics.</p>
            <ul>
              <li>Developed scalable PySpark-based ETL pipelines to replicate, transform, and load SAP HANA reporting workloads into Hadoop, Hive, and MongoDB environments, enabling advanced analytics and scalability.</li>
              <li>Built complex SQL queries for advanced analytics and integrated Python scripts to validate, clean, and transform data prior to ingestion.</li>
              <li>Optimized Spark and Hive queries for large-scale reporting, achieving over 60% reduction in query latency and improving overall system performance.</li>
              <li>Designed and implemented comprehensive data reconciliation, validation, and audit frameworks to ensure data consistency, integrity, and completeness post-migration.</li>
              <li>Automated job orchestration, monitoring, and alerting using custom PayPal frameworks and shell scripting, reducing manual intervention and improving reliability.</li>
              <li>Supported business intelligence and analytics teams in building new dashboards, reports, and self-service analytics on migrated data, driving business value and insights.</li>
              <li>Documented all migration logic, controls, and exception handling procedures for operational transparency and audit readiness.</li>
            </ul>
            <p><b>Technologies Used:</b> PySpark, HDFS, Hive, MongoDB, GIT, Shell, Custom PayPal Frameworks</p>
          </div>
        </div>
      </div>
      <!-- Project 6: IAP (Integrated Analytics Platform) -->
      <div class="col-md-12">
        <div class="card mb-3" data-aos="fade-left" data-aos-delay="200">
          <div class="card-header px-3 py-2">
            <div class="d-flex justify-content-between">
              <div>
                <h3 class="h5 mb-1">IAP (Integrated Analytics Platform)</h3>
                <div class="text-muted text-small"><b>Client:</b> AbbVie</div>
              </div><small>May 2019 – Jul 2020</small>
            </div>
          </div>
          <div class="card-body px-3 py-2">
            <p><b>Description:</b> The IAP project was designed to centralize patient data from various applications into Hadoop for analysis, processing and analyzing patient activity data to help healthcare providers make informed decisions.</p>
            <ul>
              <li>Developed ingestion pipelines for clickstream, Adobe Analytics, and transactional patient data using Apache Spark, Hive, and Hadoop, centralizing data from multiple healthcare applications.</li>
              <li>Developed Spark applications to analyze patient activity data, providing a comprehensive view of patient behavior and health outcomes.</li>
              <li>Designed and implemented scalable ETL processes to aggregate, transform, and enrich patient activity data for advanced downstream analytics and reporting.</li>
              <li>Automated data validation, error handling, and job scheduling using Autosys JIL scripting and shell scripting, improving reliability and operational efficiency.</li>
              <li>Collaborated with healthcare analysts and business users to design custom reports, dashboards, and analytics solutions for actionable patient insights.</li>
              <li>Ensured HIPAA compliance and data privacy through secure data handling, access controls, and audit logging across all ETL workflows.</li>
              <li>Documented all data flows, validation checks, and security controls to support compliance and audit requirements.</li>
            </ul>
            <p><b>Technologies Used:</b> Click Stream Data, Adobe Analytics, Apache Spark, Hadoop, Hive, Scala, Impala, Shell Scripting, Autosys</p>
          </div>
        </div>
      </div>
      <!-- Project 7: SDC (Spatial Data Collaborator) -->
      <div class="col-md-12">
        <div class="card mb-3" data-aos="fade-right" data-aos-delay="200">
          <div class="card-header px-3 py-2">
            <div class="d-flex justify-content-between">
              <div>
                <h3 class="h5 mb-1">SDC (Spatial Data Collaborator)</h3>
                <div class="text-muted text-small"><b>Client:</b> Apple</div>
              </div><small>Mar 2017 – Apr 2019</small>
            </div>
          </div>
          <div class="card-body px-3 py-2">
            <p><b>Description:</b> This project involved working with geospatial data sources to support Apple Maps; we processed and integrated data from multiple sources to create accurate and up‑to‑date maps.</p>
            <ul>
              <li>Designed and implemented Hadoop-based ETL workflows to ingest, process, and harmonize large-scale geospatial data from USGS, TIGER, and various third-party sources for Apple Maps enhancements.</li>
              <li>Developed MapReduce and Hive jobs for spatial data transformation, enrichment, validation, and quality checks, supporting advanced mapping features and analytics.</li>
              <li>Automated data validation, reconciliation, and update processes to ensure accuracy, currency, and reliability of mapping datasets.</li>
              <li>Collaborated with Apple Maps engineering and product teams to deliver timely data updates, support new map features, and resolve data quality issues.</li>
              <li>Improved data processing throughput and reduced manual intervention by implementing robust shell scripting automation and workflow monitoring.</li>
              <li>Documented ETL logic, data flows, and validation frameworks for operational transparency and knowledge sharing.</li>
            </ul>
            <p><b>Technologies Used:</b> Hadoop, Hive, HDFS, Python, Shell Scripting, MapReduce, USGS, TIGER Data Sources</p>
          </div>
        </div>
      </div>
      <!-- Project 8: Mapping Data Integration -->
      <div class="col-md-12">
        <div class="card mb-3" data-aos="fade-left" data-aos-delay="200">
          <div class="card-header px-3 py-2">
            <div class="d-flex justify-content-between">
              <div>
                <h3 class="h5 mb-1">Mapping Data Integration</h3>
                <div class="text-muted text-small"><b>Client:</b> Nokia</div>
              </div><small>Jul 2015 – Mar 2017</small>
            </div>
          </div>
          <div class="card-body px-3 py-2">
            <p><b>Description:</b> This project focused on integrating Nokia's geospatial data into Hadoop for large‑scale analytics, enabling advanced mapping and navigation solutions.</p>
            <ul>
              <li>Developed robust ingestion and transformation pipelines using Spark, Hive, and HDFS for large-scale integration of Nokia’s geospatial and mapping datasets into Hadoop-based analytics environments.</li>
              <li>Automated data quality validation, enrichment, and cleansing processes for spatial data, improving accuracy and usability for downstream analytics.</li>
              <li>Supported analytics and engineering teams in building advanced navigation, routing, and mapping solutions leveraging Hadoop-based data lakes.</li>
              <li>Optimized ETL jobs for high throughput, scalability, and reliability using Python, shell scripting, and Spark performance tuning techniques.</li>
              <li>Coordinated with Nokia’s global data teams to ensure timely data integration, updates, and resolution of data quality issues.</li>
              <li>Documented all data integration logic, validation workflows, and operational procedures for ongoing maintenance and audit readiness.</li>
            </ul>
            <p><b>Technologies Used:</b> Hadoop, Hive, HDFS, Python, Shell, Spark</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="section px-3 px-lg-4 pt-5" id="Education">
  <div class="container-narrow">
    <div class="text-center mb-5">
      <h2 class="marker marker-center">Education</h2>
    </div>

    <div class="card-header px-3 py-2">
            <div class="d-flex justify-content-between">
              <div>
                <h3 class="h5 mb-1">Jawaharlal Nehru Technological University Hyderabad</h3>
                <div class="text-muted text-small"><b>Mechanical Engineering</b></div>
              </div><small>Jun 2011 – May 2015</small>
            </div>
      </div>
    </div>
  </div>
</div>


<div class="section px-2 px-lg-4 pb-4 pt-5 mb-5" id="contact">
  <div class="container-narrow">
    <div class="text-center mb-5">
      <h2 class="marker marker-center">Contact Me</h2>
    </div>
    <div class="row">
      <div class="col-md-6" data-aos="zoom-in" data-aos-delay="100">
        <div class="bg-light my-2 p-3 pt-2">
          <div class="mt-3 px-1">
          <div class="h5">Let’s talk</div>
          <p>If you like my profile and if you have any desired opportunity then
           get in touch using my email or my contact number.</p>
          <p>See you!</p>
        </div>
          <!--<form action="https://formspree.io/your@email.com"
    method="POST">
    <div class="form-group my-2">
      <label for="name" class="form-label fw-bolder">Name</label>
      <input class="form-control" type="text" id="name" name="name" required>
    </div>
    <div class="form-group my-2">
      <label for="email" class="form-label fw-bolder">Email</label>
      <input class="form-control" type="email" id="email" name="_replyto" required>
    </div>
  <div class="form-group my-2">
    <label for="message" class="form-label fw-bolder">Message</label>
    <textarea class="form-control" style="resize: none;" id="message" name="message" rows="4" required></textarea>
  </div>
  <button class="btn btn-primary mt-2" type="submit">Send</button>
</form>
-->

<!--
<iframe src="https://docs.google.com/forms/d/e/1FAIpQLSddwFKTIyXhz-ajbuIpHiuhF8p54ptCLSgO2COMv5ED50aB0Q/viewform?embedded=true" width="640" height="645" frameborder="0" marginheight="0" marginwidth="0">Loading…</iframe>
-->




        </div>
      </div>
      <div class="col-md-6 my-2 p-3 pt-2" data-aos="fade-left" data-aos-delay="300">

        <div class="mt-53 px-1">
          <div class="row">
            <div class="col-sm-2">
            <div class="pb-1">Email:</div>
          </div>
          <div class="col-sm-10">
            <div class="pb-1 fw-bolder">vemisreenathreddy@gmail.com</div>
          </div>
          <div class="col-sm-2">
            <div class="pb-1">Phone:</div>
          </div>
          <div class="col-sm-10">
            <div class="pb-1 fw-bolder">+1 (469) 793-3883 <a href="https://wa.me/14697933883" title="+1 (469) 793-3883"><i class='fab fa-whatsapp-square'></i><span class="menu-title">WhatsApp</span></a>
            </div>
          </div>
          <div class="col-sm-2">
            <div class="pb-1">Links:</div>
          </div>
          <div class="col-sm-10">
            <div class="pb-1 fw-bolder"><a href="https://www.linkedin.com/in/sreenath-reddy-vemireddy-09bb91183" title="LinkedIn"><i class="fab fa-linkedin"></i><span>LinkedIn</span></a></div>
          </div>
 
          
          </div>
        </div>
      </div>
    </div>
  </div>
</div>
<footer class="pt-4 pb-4 text-center bg-light">
  <div class="container">
    <div class="my-3">
      <div class="h4">Sreenath Vemireddy</div>
      <p>Sr. BigData Developer</p>
      <div class="social-nav">
        <nav role="navigation">
          <ul class="nav justify-content-center">
            <li class="nav-item"><a class="nav-link" href="https://www.linkedin.com/in/sreenath-reddy-vemireddy-09bb91183" title="LinkedIn"><i class="fab fa-linkedin"></i><span class="menu-title sr-only">LinkedIn</span></a></li>
                  <li class="nav-item"><a class="nav-link" href="https://wa.me/14697933883" title="+1 (469) 793-3883"><i class='fab fa-whatsapp-square'></i><span class="menu-title sr-only">WhatsApp</span></a></li>
            <li class="nav-item"><a class="nav-link" href="https://github.com/vemireddysreenath/Resume/raw/main/SREENATH_RESUME.docx" title="Latest Resume"><i class='fas fa-download'></i><span class="menu-title sr-only">Download Resume</span></a></li>
          </ul>
        </nav>
      </div>
 <a href='https://writingbachelorthesis.com/'>writingbachelorthesis</a> <script type='text/javascript' src='https://www.freevisitorcounters.com/auth.php?id=0ad9958eadee502a86280a6b38ecf67c6c5aa847'></script>
<script type="text/javascript" src="https://www.freevisitorcounters.com/en/home/counter/943050/t/5"></script>
    </div>
    </div>
  </div>
</footer></div>
    </div>
    <div id="scrolltop"><a class="btn btn-secondary" href="#top"><span class="icon"><i class="fas fa-angle-up fa-x"></i></span></a></div>
    <script src="./scripts/imagesloaded.pkgd.min.js?ver=1.2.0"></script>
    <script src="./scripts/masonry.pkgd.min.js?ver=1.2.0"></script>
    <script src="./scripts/BigPicture.min.js?ver=1.2.0"></script>
    <script src="./scripts/purecounter.min.js?ver=1.2.0"></script>
    <script src="./scripts/bootstrap.bundle.min.js?ver=1.2.0"></script>
    <script src="./scripts/aos.min.js?ver=1.2.0"></script>
    <script src="./scripts/main.js?ver=1.2.0"></script>
  </body>
</html> 
