<!DOCTYPE html>
<!-- upto 2 directory depth--> 
<html lang="en-US">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-K8NFKJ9RBX"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-K8NFKJ9RBX');
</script>
    
    
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Sreenath profile</title>
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="crossorigin"/>
    <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Nunito+Sans:wght@300;400;700;800&amp;display=swap"/>
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Nunito+Sans:wght@300;400;700;800&amp;display=swap" media="print" onload="this.media='all'"/>
    <noscript>
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Nunito+Sans:wght@300;400;700;800&amp;display=swap"/>
    </noscript>
    <link href="./css/font-awesome/css/all.min.css?ver=1.2.0" rel="stylesheet">
    <link href="./css/bootstrap-icons/bootstrap-icons.css?ver=1.2.0" rel="stylesheet">
    <link href="./css/bootstrap.min.css?ver=1.2.0" rel="stylesheet">
    <link href="./css/aos.css?ver=1.2.0" rel="stylesheet">
    <link href="./css/main.css?ver=1.2.0" rel="stylesheet">
    <noscript>
      <style type="text/css">
        [data-aos] {
            opacity: 1 !important;
            transform: translate(0) scale(1) !important;
        }
      </style>
    </noscript>
  </head>
  <body id="top">
    <header class="bg-light">
      <nav class="navbar navbar-expand-lg navbar-light bg-light" id="header-nav" role="navigation">
        <div class="container"><a class="link-dark navbar-brand site-title mb-0" href="#">Profile</a>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
          <div class="collapse navbar-collapse" id="navbarSupportedContent">
            <ul class="navbar-nav ms-auto me-2">
              <li class="nav-item"><a class="nav-link" href="#about">About</a></li>
              <li class="nav-item"><a class="nav-link" href="#Summary">Summary</a></li>
              <li class="nav-item"><a class="nav-link" href="#skills">Skills</a></li>
              <li class="nav-item"><a class="nav-link" href="#experience">Experience</a></li>
              <li class="nav-item"><a class="nav-link" href="#Projects">Projects</a></li>
              <li class="nav-item"><a class="nav-link" href="#contact">Contact</a></li>
            </ul>
          </div>
        </div>
      </nav>
    </header>
    <div class="page-content">
      <div id="content">
<header>
  <div class="cover bg-light">
    <div class="container px-3">
      <div class="row">
        <div class="col-lg-6 p-2"><img class="img-fluid" src="images/illustrations/hello3.svg" alt="hello"/></div>
        <div class="col-lg-6">
          <div class="mt-5">
            <p class="lead text-uppercase mb-1">Hello!</p>
            <h1 class="intro-title marker" data-aos="fade-left" data-aos-delay="50">I’m Sreenath Vemireddy</h1>
            <p class="lead fw-normal mt-3" data-aos="fade-up" data-aos-delay="100">Sr. BigData Developer</p>
            <div class="social-nav" data-aos="fade-up" data-aos-delay="200">
              <nav role="navigation">
                <ul class="nav justify-content-left">
                  <li class="nav-item"><a class="nav-link" href="https://www.linkedin.com/in/sreenath-reddy-vemireddy-09bb91183" title="LinkedIn"><i class="fab fa-linkedin"></i><span class="menu-title sr-only">LinkedIn</span></a></li>
                  <li class="nav-item"><a class="nav-link" href="https://wa.me/14697933883" title="+1 (469) 793-3883"><i class='fab fa-whatsapp-square'></i><span class="menu-title sr-only">WhatsApp</span></a></li>
                  <li class="nav-item"><a class="nav-link" href="https://raw.githubusercontent.com/vemireddysreenath/vemireddysreenath.github.io/main/SREENATH_RESUME.docx" title="Latest Resume"><i class='fas fa-download'></i><span class="menu-title sr-only">Download Resume</span></a></li>

                </ul>
              </nav>
            </div>
            <div class="mt-3" data-aos="fade-up" data-aos-delay="200"><a class="btn btn-primary shadow-sm mt-1 hover-effect" href="#contact">Get In Touch <i class="fas fa-arrow-right"></i></a></div>
          </div>
        </div>
      </div>
    </div>
  </div>
  <div class="wave-bg"></div>
</header>
<div class="section pt-4 px-3 px-lg-4" id="about">
  <div class="container-narrow">
    <div class="row">
      <div class="col-md-6">
        <h2 class="h4 my-2">Hello! I’m Sreenath Vemireddy.</h2>
        <p> I am a <b>Senior Big Data and Azure Data Engineer</b> with over <b>10 years of progressive experience</b> in designing, building, and managing end-to-end data engineering solutions across diverse industries including <b>finance, healthcare, insurance, banking, and geospatial mapping</b>. My core strength lies in building <b>scalable, high-performance data pipelines</b> using cutting-edge technologies like <b>Hadoop, Spark, PySpark, and Hive</b>, and integrating them seamlessly with <b>cloud platforms such as Microsoft Azure and AWS</b>.</p>
        
      </div>
      <div class="col-md-5 offset-md-1" data-aos="fade-left" data-aos-delay="100"><img class="avatar img-fluid mt-2" src="images/avatar.png" width="400" height="400" alt="Sreenath Vemireddy"/></div>
    </div>
  </div>
</div>
<div class="section px-3 px-lg-4 pt-5" id="Summary">
  <div class="container-narrow">
    <div class="text-center mb-5">
      <h2 class="marker marker-center">Proficianal Summary</h2>
    </div>
    <div class="text-center">
        <p class="mx-auto mb-3" style="max-width:850px"> I am a <b>Senior Big Data and Azure Data Engineer</b> with over <b>10 years of progressive experience</b> in designing, building, and managing end-to-end data engineering solutions across diverse industries including <b>finance, healthcare, insurance, banking, and geospatial mapping</b>. My core strength lies in building <b>scalable, high-performance data pipelines</b> using cutting-edge technologies like <b>Hadoop, Spark, PySpark, and Hive</b>, and integrating them seamlessly with <b>cloud platforms such as Microsoft Azure and AWS</b>.</p>
        <p class="mx-auto mb-3" style="max-width:850px"> I have a proven track record in delivering <b>enterprise-level data migrations</b>—moving terabytes of data from legacy systems to cloud environments—while ensuring <b>data quality, lineage tracking, regulatory compliance</b>, and performance optimization. I specialize in <b>metadata-driven automation</b> frameworks for <b>schema validation, ingestion, transformation, audit logging, and alerting</b>, significantly reducing manual effort and enhancing scalability.</p>
        <p class="mx-auto mb-3" style="max-width:850px"> My expertise includes tools like <b>Azure Data Factory (ADF)</b>, <b>Azure Blob Storage</b>, <b>Azure SQL Database</b>, <b>Azure Key Vault</b>, and orchestration tools like <b>Apache Airflow, Zena, and Autosys</b>. I have implemented <b>complex DAGs</b> for ETL pipelines, automated failure handling, and ensured secure cloud integration with strict governance practices.</p>
        <p class="mx-auto mb-3" style="max-width:850px"> I’ve collaborated with top-tier global organizations such as <b>DBS Bank, PayPal, ICICI Bank, AbbVie, Country Financial, Apple, and Nokia</b>. My work has contributed to mission-critical systems including <b>MAS 637 and PILLAR3 regulatory reporting</b>, <b>SAS exit transformations</b>, <b>CRM cloud migrations</b>, <b>HANA-to-Hadoop reporting</b>, and <b>spatial data integration for Apple and Nokia Maps</b>.</p>
        <p class="mx-auto mb-3" style="max-width:850px"> I am well-versed in <b>data governance</b> practices, using tools like <b>Collibra</b> to manage metadata and ensure compliance. I regularly optimize performance of <b>Spark jobs, Hive queries, and ADF pipelines</b> and have utilized <b>Power BI, Adobe Analytics, and Apache Superset</b> for data visualization and reporting.</p>
        <p class="mx-auto mb-3" style="max-width:850px"> My approach is guided by <b>Agile methodologies</b> and <b>DevOps principles</b>, including <b>CI/CD automation with Git and Azure DevOps</b>. I’ve also taken on <b>leadership roles</b>, mentoring junior engineers, conducting code reviews, and guiding teams in the delivery of scalable and robust solutions.</p>
        <p class="mx-auto mb-3" style="max-width:850px"> I am passionate about <b>transforming raw data into actionable intelligence</b> and continuously strive to enhance data value, accessibility, and governance across organizations. For me, data engineering is not just about infrastructure—it's about enabling smarter, data-driven decisions that drive business success.</p>
    </div>
    <!-- Removed the symbol code -->
    </div>
  </div>
</div>
<div class="section px-3 px-lg-4 pt-5" id="skills">
  <div class="container-narrow">
    <div class="text-center mb-5">
      <h2 class="marker marker-center">My Skills</h2>
    </div>
    <!-- commented 
    <div class="text-center">
      <p class="mx-auto mb-3" style="max-width:600px">I am a quick learner and specialize in multitude of skills required for Web Application Development and Product Design</p>
    </div>
    Commented-->
    <div class="bg-light p-3">
      <div class="row">
        <div class="col-md-50">
        
          <!-- new lines added for skill -->
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">Hadoop, Spark, PySpark</span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">Cloudera, Hortonworks</span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">Python, Scala, Core Java, SQL, Shell Scripting</span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">Hue, Apache Superset, Hive, Impala, Pig, Sqoop</span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">Azure (Data Factory, Blob Storage, SQL Database, Key Vault)</span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">AWS (S3, Ingestion Pipelines)</span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">Apache Airflow, Zena, Autosys (JIL scripting)</span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">MySQL, Oracle, SQL Server, Vertica, SAP HANA, MongoDB</span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">Adobe Analytics, Power BI, SAS, Watch UI</span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">PyCharm, IntelliJ, Eclipse, Jupyter Notebook, Azure DevOps</span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">Git, GitHub, Bitbucket</span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">Metadata-Driven Automation Frameworks, ADA Framework</span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">CI/CD Pipelines, Agile Delivery, DevOps Practices</span></div>
          </div>
          <div class="py-1">
            <div class="text-center fw-bolder"><span class="me-auto">Collibra (Data Governance), MAS 637, PILLAR3 Reporting</span></div>
          </div>
          

        </div>
      </div>
    </div>
  </div>
</div>

<div class="section px-3 px-lg-4 pt-5" id="experience">
  <div class="container-narrow">
    <div class="text-center mb-5">
      <h2 class="marker marker-center">Experience</h2>
    </div>
    <div class="row">
      <div class="col-md-6">
        <div class="card mb-3" data-aos="fade-right" data-aos-delay="100">
          <div class="card-header px-3 py-2">
            <div class="d-flex justify-content-between">
              <div>
                <h3 class="h5 mb-1">Hadoop Lead</h3>
                <div class="text-muted text-small">FirstMeridian Business Services Ltd (Client: Country Financial), India</div>
              </div><small>Jan 2024 – Mar 2025</small>
            </div>
          </div>
          <div class="card-body px-3 py-2">
            <p>Led the development of a metadata-driven data migration solution from Guidewire S3 to Azure SQL, implementing SCD2 methodology and optimizing data pipelines using Azure Data Factory and Hadoop ecosystems.</p>
          </div>
        </div>
      </div>
      
      <div class="col-md-6">
        <div class="card mb-3" data-aos="fade-left" data-aos-delay="300">
          <div class="card-header px-3 py-2">
            <div class="d-flex justify-content-between">
              <div>
                <h3 class="h5 mb-1">Consultant (Big Data Engineer)</h3>
                <div class="text-muted text-small">U3 InfoTech Pte Ltd (Client: DBS Bank), Singapore</div>
              </div><small>Jun 2022 – Jan 2024</small>
            </div>
          </div>
          <div class="card-body px-3 py-2">
            <p>Worked on MAS 637 and Pillar3 Reporting projects for regulatory compliance and risk evaluation. Led SAS Exit project to migrate legacy SAS scripts into the ADA platform using PySpark and Airflow.</p>
          </div>
        </div>
      </div>
      
      <div class="col-md-6">
        <div class="card mb-3" data-aos="fade-right" data-aos-delay="200">
          <div class="card-header px-3 py-2">
            <div class="d-flex justify-content-between">
              <div>
                <h3 class="h5 mb-1">Consultant – Big Data Migration</h3>
                <div class="text-muted text-small">Ernst & Young (Clients: PayPal & ICICI), Chennai, India</div>
              </div><small>Jul 2020 – Jun 2022</small>
            </div>
          </div>
          <div class="card-body px-3 py-2">
            <p>Delivered end-to-end CRM data migration to Azure and HANA to Hadoop report migrations, improving system performance and data delivery pipelines across cloud and big data platforms.</p>
          </div>
        </div>
      </div>
      
      <div class="col-md-6">
        <div class="card mb-3" data-aos="fade-left" data-aos-delay="400">
          <div class="card-header px-3 py-2">
            <div class="d-flex justify-content-between">
              <div>
                <h3 class="h5 mb-1">Software Engineer – Data Analytics</h3>
                <div class="text-muted text-small">ExperisIT Pvt Ltd (Client: AbbVie), Chennai, India</div>
              </div><small>May 2019 – Jul 2020</small>
            </div>
          </div>
          <div class="card-body px-3 py-2">
            <p>Developed Spark-based applications for centralized patient data analytics on Hadoop platform to support data-driven decision making in the healthcare domain.</p>
          </div>
        </div>
      </div>
      
      <div class="col-md-6">
        <div class="card mb-3" data-aos="fade-right" data-aos-delay="300">
          <div class="card-header px-3 py-2">
            <div class="d-flex justify-content-between">
              <div>
                <h3 class="h5 mb-1">Engineer – Geospatial Data Solutions</h3>
                <div class="text-muted text-small">RMSI Pvt Ltd (Client: Apple), Hyderabad, India</div>
              </div><small>Mar 2017 – Apr 2019</small>
            </div>
          </div>
          <div class="card-body px-3 py-2">
            <p>Ingested and processed large volumes of geospatial data for Apple’s Maps platform using Hadoop and Shell, enhancing spatial data accuracy and performance.</p>
          </div>
        </div>
      </div>
      
      <div class="col-md-6">
        <div class="card mb-3" data-aos="fade-left" data-aos-delay="500">
          <div class="card-header px-3 py-2">
            <div class="d-flex justify-content-between">
              <div>
                <h3 class="h5 mb-1">Trainee Engineer – Data Engineering</h3>
                <div class="text-muted text-small">TriGeo Technologies Pvt Ltd (Client: Nokia), Hyderabad, India</div>
              </div><small>Jul 2015 – Mar 2017</small>
            </div>
          </div>
          <div class="card-body px-3 py-2">
            <p>Assisted in building data ingestion pipelines for large-scale geospatial data integration into Hadoop, supporting Nokia's navigation and mapping initiatives.</p>
          </div>
        </div>
      </div>
      
      

      
      
    </div>
  </div>
</div>
<div class="section px-3 px-lg-4 pt-5" id="Projects">
  <div class="container-narrow">
    <div class="text-center mb-5">
      <h2 class="marker marker-center">Latest Projects</h2>
    </div>
    <div class="row">


      <div class="col-md-12">
        <div class="card mb-3" data-aos="fade-right" data-aos-delay="200">
          <div class="card-header px-3 py-2">
            <div class="d-flex justify-content-between">
              <div>
                <h3 class="h5 mb-1">Comm-Agg End-to-End Data Migration Solution</h3>
                <div class="text-muted text-small"><b>Client:</b> Country Financial</div>
              </div><small>Jan 2024 – Mar 2025</small>
            </div>
          </div>
          <div class="card-body px-3 py-2">
            <p><b>Description:</b> This project focused on developing a complete end-to-end solution for migrating data from Guidewire S3 to Azure SQL, using Blob Storage as an intermediary. The goal was to ensure high-quality, scalable data pipelines that adhered to the Slowly Changing Dimension Type 2 (SCD2) methodology for historical tracking. Automation and metadata-driven processes played a vital role in minimizing manual efforts while ensuring high data integrity. Python scripts were created for robust error handling, email notifications, and data comparison reports between Hive and ODS tables, ensuring precise validation. The final solution involved integrating Azure SQL data into Hadoop and processing it through multiple layers to prepare it for business consumption.</p>
            <p><b>Technologies:</b> Azure Data Factory (ADF), Blob Storage, Azure SQL, Python, Hadoop, Hive, Impala, Zena, Shell Scripting</p>
            <p><b>Responsibilities:</b><br>
              &nbsp;&nbsp;&nbsp;• Developed and optimized data pipelines in ADF to extract data from Guidewire S3 and load into Azure SQL.<br>
              &nbsp;&nbsp;&nbsp;• Implemented SCD2 for effective historical tracking of data changes.<br>
              &nbsp;&nbsp;&nbsp;• Designed and deployed automated incremental data load processes to reduce manual efforts.<br>
              &nbsp;&nbsp;&nbsp;• Automated schema management using metadata-driven frameworks.<br>
              &nbsp;&nbsp;&nbsp;• Created Python scripts for error detection, email alerts, and data validation reports.<br>
              &nbsp;&nbsp;&nbsp;• Migrated data from Azure SQL to Hadoop ecosystem and maintained structured zones: landing, core, and active views.<br>
              &nbsp;&nbsp;&nbsp;• Orchestrated pipeline execution and stored procedures using Zena for improved automation and monitoring.</p>
          </div>
        </div>
      </div>
      <div class="col-md-12">
        <div class="card mb-3" data-aos="fade-left" data-aos-delay="200">
          <div class="card-header px-3 py-2">
            <div class="d-flex justify-content-between">
              <div>
                <h3 class="h5 mb-1">MAS 637 and PILLAR3 Reporting</h3>
                <div class="text-muted text-small"><b>Client:</b> DBS Bank</div>
              </div><small>Jun 2022 – Jan 2024</small>
            </div>
          </div>
          <div class="card-body px-3 py-2">
            <p><b>Description:</b> The MAS 637 project involved generating reports for the Monetary Authority of Singapore (MAS) to ensure regulatory compliance. This project analyzed pool statuses across various user types and tracked the changes over a 12-month period. PILLAR3 reporting focused on risk management, using predictive analytics to evaluate user behavior and default risks based on historical data.</p>
            <p><b>Technologies:</b> ADA (In-house framework), PySpark, Spark SQL, Presto, Hive, Hadoop, Airflow, Jupyter, Collibra</p>
            <p><b>Responsibilities:</b><br>
              &nbsp;&nbsp;&nbsp;• Developed and optimized PySpark scripts within the ADA (Advanced DBS Analytics) framework to analyze user pool statuses across different time periods.<br>
              &nbsp;&nbsp;&nbsp;• Automated trend analysis and default risk evaluations for users, improving regulatory compliance and decision-making.<br>
              &nbsp;&nbsp;&nbsp;• Provided insights into financial transactions and default status for users by integrating various data sources within the bank’s system.<br>
              &nbsp;&nbsp;&nbsp;• Delivered high-quality, compliant reports to the Monetary Authority of Singapore, ensuring that data and analysis met regulatory standards.<br>
              &nbsp;&nbsp;&nbsp;• Applied predictive analytics techniques to evaluate potential default risks, improving the bank's risk management strategies.<br>
              &nbsp;&nbsp;&nbsp;• Enhanced the reporting process by implementing Presto for faster data retrieval and analysis.<br>
              &nbsp;&nbsp;&nbsp;• Created and managed metadata structures within Collibra to support proper data governance, lineage tracking, and regulatory compliance.</p>
          </div>
        </div>
      </div>
      <div class="col-md-12">
        <div class="card mb-3" data-aos="fade-right" data-aos-delay="200">
          <div class="card-header px-3 py-2">
            <div class="d-flex justify-content-between">
              <div>
                <h3 class="h5 mb-1">SAS Exit</h3>
                <div class="text-muted text-small"><b>Client:</b> DBS Bank</div>
              </div><small>Jun 2022 – Jan 2024</small>
            </div>
          </div>
          <div class="card-body px-3 py-2">
            <p><b>Description:</b> The SAS Exit project was designed to migrate legacy SAS scripts into DBS Bank’s ADA platform. The migration involved converting complex SAS reports into PySpark-based solutions, improving the performance and scalability of the reporting system.</p>
            <p><b>Technologies:</b> SAS, ADA (In-house framework), PySpark, Airflow, Collibra, Python, Spark SQL, Presto, Hive, Hadoop</p>
            <p><b>Responsibilities:</b><br>
              &nbsp;&nbsp;&nbsp;• Led the migration of complex SAS reporting scripts to PySpark within the ADA framework, significantly improving the performance of data processing.<br>
              &nbsp;&nbsp;&nbsp;• Designed a robust architecture for the ADA platform that met the requirements of the SAS reports and ensured seamless integration with existing systems.<br>
              &nbsp;&nbsp;&nbsp;• Developed and maintained metadata structures in Collibra, improving data governance and ensuring compliance.<br>
              &nbsp;&nbsp;&nbsp;• Automated the execution of PySpark jobs using Apache Airflow, ensuring timely report generation and reducing manual intervention.<br>
              &nbsp;&nbsp;&nbsp;• Collaborated with key stakeholders to understand business requirements and ensure the migration of reports aligned with their expectations.</p>
          </div>
        </div>
      </div>
      <div class="col-md-12">
        <div class="card mb-3" data-aos="fade-left" data-aos-delay="200">
          <div class="card-header px-3 py-2">
            <div class="d-flex justify-content-between">
              <div>
                <h3 class="h5 mb-1">CRM Data Migration</h3>
                <div class="text-muted text-small"><b>Client:</b> PayPal & ICICI</div>
              </div><small>Jul 2020 – Jun 2022</small>
            </div>
          </div>
          <div class="card-body px-3 py-2">
            <p><b>Description:</b> This project involved migrating CRM data from multiple on-premises and cloud environments to an Azure-based platform. The goal was to ensure smooth data transfer while generating insightful reports for the business.</p>
            <p><b>Technologies:</b> Azure Data Factory (ADF), Blob Storage, SQL Database, Oracle, Key-Vault, Vertica, Azure Logic Apps</p>
            <p><b>Responsibilities:</b><br>
              &nbsp;&nbsp;&nbsp;• Led the migration of CRM data to Azure SQL using Azure Data Factory (ADF), ensuring reliable and efficient data processing.<br>
              &nbsp;&nbsp;&nbsp;• Developed scalable ADF pipelines to handle data ingestion from multiple sources, including Vertica and Oracle databases.<br>
              &nbsp;&nbsp;&nbsp;• Managed a team of junior engineers, providing guidance on best practices for cloud migration and data governance.<br>
              &nbsp;&nbsp;&nbsp;• Automated the scheduling of data transfer tasks using Azure Logic Apps, improving overall efficiency and reducing delays in data processing.</p>
          </div>
        </div>
      </div>
      <div class="col-md-12">
        <div class="card mb-3" data-aos="fade-right" data-aos-delay="200">
          <div class="card-header px-3 py-2">
            <div class="d-flex justify-content-between">
              <div>
                <h3 class="h5 mb-1">H2H (HANA to Hadoop)</h3>
                <div class="text-muted text-small"><b>Client:</b> PayPal</div>
              </div><small>Jul 2020 – Jun 2022</small>
            </div>
          </div>
          <div class="card-body px-3 py-2">
            <p><b>Description:</b> The H2H project involved migrating SAP HANA reports to a Hadoop-based environment, ensuring improved performance and scalability for reporting and analytics.</p>
            <p><b>Technologies:</b> PySpark, HDFS, Hive, MongoDB, GIT, Shell, Custom PayPal Frameworks</p>
            <p><b>Responsibilities:</b><br>
              &nbsp;&nbsp;&nbsp;• Led the design and migration of SAP HANA reports to the Hadoop ecosystem, focusing on Spark and Hive for data processing.<br>
              &nbsp;&nbsp;&nbsp;• Developed Spark and Hive-based reports to improve data accessibility and reporting performance.<br>
              &nbsp;&nbsp;&nbsp;• Utilized Hadoop, HDFS, and MongoDB to store and manage large volumes of data, improving data storage and processing efficiency.<br>
              &nbsp;&nbsp;&nbsp;• Collaborated with business teams to deliver reports through various channels, including email, web UI, and dashboards.<br>
              &nbsp;&nbsp;&nbsp;• Optimized the data migration process to ensure accurate and timely delivery of reports to end-users.</p>
          </div>
        </div>
      </div>
      <div class="col-md-12">
        <div class="card mb-3" data-aos="fade-left" data-aos-delay="200">
          <div class="card-header px-3 py-2">
            <div class="d-flex justify-content-between">
              <div>
                <h3 class="h5 mb-1">IAP (Integrated Analytics Platform)</h3>
                <div class="text-muted text-small"><b>Client:</b> AbbVie</div>
              </div><small>May 2019 – Jul 2020</small>
            </div>
          </div>
          <div class="card-body px-3 py-2">
            <p><b>Description:</b> The IAP project was designed to centralize patient data from various applications into Hadoop for analysis. The goal was to process and analyze patient activity data, allowing healthcare providers to make informed decisions.</p>
            <p><b>Technologies:</b> Apache Spark, Hadoop, Hive, Scala, Impala, Shell Scripting, Autosys</p>
            <p><b>Responsibilities:</b><br>
              &nbsp;&nbsp;&nbsp;• Ingested data from various source applications into Hadoop, ensuring seamless integration and data processing.<br>
              &nbsp;&nbsp;&nbsp;• Developed Spark applications to analyze patient activity data, providing a comprehensive view of patient behavior and outcomes.<br>
              &nbsp;&nbsp;&nbsp;• Built efficient Hive and Impala queries to transform and analyze the data, enabling the creation of actionable insights for healthcare providers.<br>
              &nbsp;&nbsp;&nbsp;• Automated batch data processing using Autosys, ensuring timely updates and reducing manual interventions.<br>
              &nbsp;&nbsp;&nbsp;• Managed the end-to-end pipeline for data ingestion, processing, and reporting.</p>
          </div>
        </div>
      </div>
      <div class="col-md-12">
        <div class="card mb-3" data-aos="fade-right" data-aos-delay="200">
          <div class="card-header px-3 py-2">
            <div class="d-flex justify-content-between">
              <div>
                <h3 class="h5 mb-1">SDC (Spatial Data Collaborator)</h3>
                <div class="text-muted text-small"><b>Client:</b> Apple</div>
              </div><small>Mar 2017 – Apr 2019</small>
            </div>
          </div>
          <div class="card-body px-3 py-2">
            <p><b>Description:</b> This project involved working with geospatial data sources to support the development of Apple’s map features. We processed and integrated data from multiple sources to create accurate and up-to-date maps.</p>
            <p><b>Technologies:</b> Hadoop, Hive, HDFS, Python, Shell Scripting, MapReduce, USGS, TIGER Data Sources</p>
            <p><b>Responsibilities:</b><br>
              &nbsp;&nbsp;&nbsp;• Ingested geospatial data from multiple sources (USGS, TIGER) into Hadoop for processing and integration into Apple’s mapping systems.<br>
              &nbsp;&nbsp;&nbsp;• Developed Hadoop-based pipelines to process large geospatial datasets, enabling more accurate and comprehensive map features.<br>
              &nbsp;&nbsp;&nbsp;• Worked with Apple’s mapping team to ensure the geospatial data was accurately merged, improving map development.<br>
              &nbsp;&nbsp;&nbsp;• Automated the data ingestion process from SFTP servers, reducing manual data processing time.</p>
          </div>
        </div>
      </div>
      <div class="col-md-12">
        <div class="card mb-3" data-aos="fade-left" data-aos-delay="200">
          <div class="card-header px-3 py-2">
            <div class="d-flex justify-content-between">
              <div>
                <h3 class="h5 mb-1">Mapping Data Integration</h3>
                <div class="text-muted text-small"><b>Client:</b> Nokia</div>
              </div><small>Jul 2015 – Mar 2017</small>
            </div>
          </div>
          <div class="card-body px-3 py-2">
            <p><b>Description:</b> This project focused on integrating Nokia's geospatial data into Hadoop for large-scale analytics, enabling the creation of advanced mapping and navigation solutions.</p>
            <p><b>Technologies:</b> Hadoop, Hive, HDFS, Python, Shell, Spark</p>
            <p><b>Responsibilities:</b><br>
              &nbsp;&nbsp;&nbsp;• Assisted in the integration of Nokia's geospatial data into Hadoop for large-scale analysis.<br>
              &nbsp;&nbsp;&nbsp;• Worked on developing data ingestion pipelines to process geospatial data from various sources, improving data quality and consistency.<br>
              &nbsp;&nbsp;&nbsp;• Contributed to developing tools to automate the ingestion process, reducing manual effort and improving operational efficiency.</p>
          </div>
        </div>
      </div>

          
        </div>
      </div>
    </div>
  </div>
</div>
<div class="section px-2 px-lg-4 pb-4 pt-5 mb-5" id="contact">
  <div class="container-narrow">
    <div class="text-center mb-5">
      <h2 class="marker marker-center">Contact Me</h2>
    </div>
    <div class="row">
      <div class="col-md-6" data-aos="zoom-in" data-aos-delay="100">
        <div class="bg-light my-2 p-3 pt-2">
          <div class="mt-3 px-1">
          <div class="h5">Let’s talk</div>
          <p>If you like my profile and if you have any desired opportunity then
           get in touch using my email or my contact number.</p>
          <p>See you!</p>
        </div>
          <!--<form action="https://formspree.io/your@email.com"
    method="POST">
    <div class="form-group my-2">
      <label for="name" class="form-label fw-bolder">Name</label>
      <input class="form-control" type="text" id="name" name="name" required>
    </div>
    <div class="form-group my-2">
      <label for="email" class="form-label fw-bolder">Email</label>
      <input class="form-control" type="email" id="email" name="_replyto" required>
    </div>
  <div class="form-group my-2">
    <label for="message" class="form-label fw-bolder">Message</label>
    <textarea class="form-control" style="resize: none;" id="message" name="message" rows="4" required></textarea>
  </div>
  <button class="btn btn-primary mt-2" type="submit">Send</button>
</form>
-->

<!--
<iframe src="https://docs.google.com/forms/d/e/1FAIpQLSddwFKTIyXhz-ajbuIpHiuhF8p54ptCLSgO2COMv5ED50aB0Q/viewform?embedded=true" width="640" height="645" frameborder="0" marginheight="0" marginwidth="0">Loading…</iframe>
-->




        </div>
      </div>
      <div class="col-md-6 my-2 p-3 pt-2" data-aos="fade-left" data-aos-delay="300">

        <div class="mt-53 px-1">
          <div class="row">
            <div class="col-sm-2">
            <div class="pb-1">Email:</div>
          </div>
          <div class="col-sm-10">
            <div class="pb-1 fw-bolder">vemisreenathreddy@gmail.com</div>
          </div>
          <div class="col-sm-2">
            <div class="pb-1">Phone:</div>
          </div>
          <div class="col-sm-10">
            <div class="pb-1 fw-bolder">+1 (469) 793-3883 <a href="https://wa.me/14697933883" title="+1 (469) 793-3883"><i class='fab fa-whatsapp-square'></i><span class="menu-title">WhatsApp</span></a>
            </div>
          </div>
          <div class="col-sm-2">
            <div class="pb-1">Links:</div>
          </div>
          <div class="col-sm-10">
            <div class="pb-1 fw-bolder"><a href="https://www.linkedin.com/in/sreenath-reddy-vemireddy-09bb91183" title="LinkedIn"><i class="fab fa-linkedin"></i><span>LinkedIn</span></a></div>
          </div>
 
          
          </div>
        </div>
      </div>
    </div>
  </div>
</div>
<footer class="pt-4 pb-4 text-center bg-light">
  <div class="container">
    <div class="my-3">
      <div class="h4">Sreenath Vemireddy</div>
      <p>Sr. BigData Developer</p>
      <div class="social-nav">
        <nav role="navigation">
          <ul class="nav justify-content-center">
            <li class="nav-item"><a class="nav-link" href="https://www.linkedin.com/in/sreenath-reddy-vemireddy-09bb91183" title="LinkedIn"><i class="fab fa-linkedin"></i><span class="menu-title sr-only">LinkedIn</span></a></li>
                  <li class="nav-item"><a class="nav-link" href="https://wa.me/14697933883" title="+1 (469) 793-3883"><i class='fab fa-whatsapp-square'></i><span class="menu-title sr-only">WhatsApp</span></a></li>
            <li class="nav-item"><a class="nav-link" href="https://github.com/vemireddysreenath/Resume/raw/main/SREENATH_RESUME.docx" title="Latest Resume"><i class='fas fa-download'></i><span class="menu-title sr-only">Download Resume</span></a></li>
          </ul>
        </nav>
      </div>
 <a href='https://writingbachelorthesis.com/'>writingbachelorthesis</a> <script type='text/javascript' src='https://www.freevisitorcounters.com/auth.php?id=0ad9958eadee502a86280a6b38ecf67c6c5aa847'></script>
<script type="text/javascript" src="https://www.freevisitorcounters.com/en/home/counter/943050/t/5"></script>
    </div>
    </div>
  </div>
</footer></div>
    </div>
    <div id="scrolltop"><a class="btn btn-secondary" href="#top"><span class="icon"><i class="fas fa-angle-up fa-x"></i></span></a></div>
    <script src="./scripts/imagesloaded.pkgd.min.js?ver=1.2.0"></script>
    <script src="./scripts/masonry.pkgd.min.js?ver=1.2.0"></script>
    <script src="./scripts/BigPicture.min.js?ver=1.2.0"></script>
    <script src="./scripts/purecounter.min.js?ver=1.2.0"></script>
    <script src="./scripts/bootstrap.bundle.min.js?ver=1.2.0"></script>
    <script src="./scripts/aos.min.js?ver=1.2.0"></script>
    <script src="./scripts/main.js?ver=1.2.0"></script>
  </body>
</html> 
